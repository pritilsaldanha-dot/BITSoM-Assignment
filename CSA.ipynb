{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation & Purchase Prediction\n",
    "## Module 4 Assignment - Masai School\n",
    "\n",
    "### Learning Path for Beginners üéì\n",
    "\n",
    "This notebook will guide you through your first data science project! We'll learn:\n",
    "1. **Data Exploration** - Understanding your data\n",
    "2. **Preprocessing** - Cleaning and preparing data\n",
    "3. **Clustering** - Grouping similar customers\n",
    "4. **Prediction** - Building ML models\n",
    "5. **Optimization** - Making models better\n",
    "\n",
    "**Total Points: 100**\n",
    "- Part 1: Data Exploration & Preprocessing (20 points)\n",
    "- Part 2: Customer Segmentation (25 points)\n",
    "- Part 3: Predictive Modeling (35 points)\n",
    "- Part 4: Model Optimization (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import Libraries\n",
    "\n",
    "First, let's import all the tools we need. Think of these as your toolbox!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd  # For working with tables (DataFrames)\n",
    "import numpy as np   # For mathematical operations\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt  # For creating plots\n",
    "import seaborn as sns           # For beautiful statistical plots\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             silhouette_score)\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Data Exploration & Preprocessing (20 points)\n",
    "\n",
    "## üéØ Learning Goals:\n",
    "- Load and understand your dataset\n",
    "- Find and fix missing values\n",
    "- Detect and handle outliers\n",
    "- Create visualizations to understand data patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('customer_data.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\\n\")\n",
    "print(f\"üìä Dataset Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(\"=\"*50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Understanding the Data Structure\n",
    "\n",
    "**What does `.info()` tell us?**\n",
    "- Number of rows and columns\n",
    "- Data types of each column (numbers, text, etc.)\n",
    "- How many non-null (non-empty) values each column has\n",
    "- Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Statistical Summary\n",
    "\n",
    "**What does `.describe()` show?**\n",
    "- `count`: How many values\n",
    "- `mean`: Average value\n",
    "- `std`: Standard deviation (how spread out the data is)\n",
    "- `min/max`: Smallest and largest values\n",
    "- `25%, 50%, 75%`: Quartiles (data distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Statistical Summary of Numerical Features:\")\n",
    "print(\"=\"*50)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Check for Missing Values\n",
    "\n",
    "**Why are missing values important?**\n",
    "- Machine learning models can't work with missing data\n",
    "- Missing data can indicate patterns (e.g., customers not providing info)\n",
    "- We need to decide: fill them, remove them, or use them as information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Create a summary DataFrame\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Values': missing_values.values,\n",
    "    'Percentage': missing_percent.values\n",
    "})\n",
    "\n",
    "# Only show columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "print(\"=\"*50)\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "    print(f\"\\n‚ö†Ô∏è Total columns with missing values: {len(missing_df)}\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Visualize Missing Values\n",
    "\n",
    "A heatmap helps us see patterns in missing data visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a missing value heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.isnull(), cbar=True, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values Heatmap\\n(Yellow = Missing, Purple = Present)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Columns')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Handle Missing Values\n",
    "\n",
    "**Strategy:**\n",
    "- **Numerical columns**: Fill with median (middle value - less affected by outliers)\n",
    "- **Categorical columns**: Fill with mode (most frequent value)\n",
    "\n",
    "**Why median for numbers?**\n",
    "- Mean can be skewed by extreme values\n",
    "- Median represents the \"typical\" value better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove customer_id and target variable from processing\n",
    "if 'customer_id' in numerical_cols:\n",
    "    numerical_cols.remove('customer_id')\n",
    "if 'high_value_customer' in numerical_cols:\n",
    "    numerical_cols.remove('high_value_customer')\n",
    "if 'customer_id' in categorical_cols:\n",
    "    categorical_cols.remove('customer_id')\n",
    "\n",
    "print(f\"üìä Numerical columns: {len(numerical_cols)}\")\n",
    "print(numerical_cols)\n",
    "print(f\"\\nüìù Categorical columns: {len(categorical_cols)}\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# Fill missing values\n",
    "print(\"\\nüîß Filling missing values...\")\n",
    "\n",
    "# Numerical: Fill with median\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median_value = df[col].median()\n",
    "        df[col].fillna(median_value, inplace=True)\n",
    "        print(f\"  ‚úì {col}: Filled {df[col].isnull().sum()} values with median = {median_value:.2f}\")\n",
    "\n",
    "# Categorical: Fill with mode\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"  ‚úì {col}: Filled {df[col].isnull().sum()} values with mode = {mode_value}\")\n",
    "\n",
    "print(\"\\n‚úÖ All missing values handled!\")\n",
    "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Detect Outliers\n",
    "\n",
    "**What are outliers?**\n",
    "- Values that are very different from most other values\n",
    "- Example: If most customers spend ‚Çπ100-500, but one spends ‚Çπ50,000\n",
    "\n",
    "**Why detect them?**\n",
    "- They can mess up our models\n",
    "- But sometimes they're important (VIP customers!)\n",
    "\n",
    "**Method: IQR (Interquartile Range)**\n",
    "- Q1 = 25th percentile\n",
    "- Q3 = 75th percentile\n",
    "- IQR = Q3 - Q1\n",
    "- Outliers: Values < Q1 - 1.5√óIQR OR > Q3 + 1.5√óIQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method\n",
    "    Returns: lower bound, upper bound, outlier indices\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    return lower_bound, upper_bound, outliers.index\n",
    "\n",
    "# Check outliers for key numerical columns\n",
    "print(\"üîç Outlier Detection (IQR Method):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "outlier_summary = []\n",
    "for col in ['total_spend', 'num_transactions', 'avg_transaction_value', 'days_since_last_purchase']:\n",
    "    lower, upper, outlier_idx = detect_outliers_iqr(df, col)\n",
    "    outlier_count = len(outlier_idx)\n",
    "    outlier_percent = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outliers': outlier_count,\n",
    "        'Percentage': f\"{outlier_percent:.2f}%\",\n",
    "        'Lower Bound': f\"{lower:.2f}\",\n",
    "        'Upper Bound': f\"{upper:.2f}\"\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Visualize Outliers with Box Plots\n",
    "\n",
    "**How to read a box plot:**\n",
    "- Box: Middle 50% of data (Q1 to Q3)\n",
    "- Line in box: Median\n",
    "- Whiskers: Extend to min/max within 1.5√óIQR\n",
    "- Dots beyond whiskers: Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for key numerical features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Box Plots: Outlier Detection', fontsize=16, fontweight='bold')\n",
    "\n",
    "cols_to_plot = ['total_spend', 'num_transactions', 'avg_transaction_value', 'days_since_last_purchase']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'plum']\n",
    "\n",
    "for idx, (col, color) in enumerate(zip(cols_to_plot, colors)):\n",
    "    row = idx // 2\n",
    "    col_pos = idx % 2\n",
    "    \n",
    "    axes[row, col_pos].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
    "                                boxprops=dict(facecolor=color),\n",
    "                                medianprops=dict(color='red', linewidth=2))\n",
    "    axes[row, col_pos].set_title(col.replace('_', ' ').title(), fontweight='bold')\n",
    "    axes[row, col_pos].set_ylabel('Value')\n",
    "    axes[row, col_pos].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Distribution Plots\n",
    "\n",
    "**Why visualize distributions?**\n",
    "- See if data is normally distributed (bell curve)\n",
    "- Identify skewness (data leaning left or right)\n",
    "- Spot multiple peaks (might indicate different customer groups!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Distribution of Key Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, col in enumerate(cols_to_plot):\n",
    "    row = idx // 2\n",
    "    col_pos = idx % 2\n",
    "    \n",
    "    axes[row, col_pos].hist(df[col].dropna(), bins=50, color=colors[idx], alpha=0.7, edgecolor='black')\n",
    "    axes[row, col_pos].set_title(col.replace('_', ' ').title(), fontweight='bold')\n",
    "    axes[row, col_pos].set_xlabel('Value')\n",
    "    axes[row, col_pos].set_ylabel('Frequency')\n",
    "    axes[row, col_pos].axvline(df[col].median(), color='red', linestyle='--', linewidth=2, label='Median')\n",
    "    axes[row, col_pos].axvline(df[col].mean(), color='green', linestyle='--', linewidth=2, label='Mean')\n",
    "    axes[row, col_pos].legend()\n",
    "    axes[row, col_pos].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Correlation Analysis\n",
    "\n",
    "**What is correlation?**\n",
    "- Measures how two variables move together\n",
    "- Range: -1 to +1\n",
    "  - +1: Perfect positive (both increase together)\n",
    "  - 0: No relationship\n",
    "  - -1: Perfect negative (one increases, other decreases)\n",
    "\n",
    "**Why is it important?**\n",
    "- Find which features are related to your target (high_value_customer)\n",
    "- Detect multicollinearity (features too similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns for correlation\n",
    "numerical_features = df[numerical_cols + ['high_value_customer']]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numerical_features.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix\\n(Closer to 1 or -1 = Stronger Relationship)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation with target variable\n",
    "print(\"\\nüéØ Correlation with Target Variable (high_value_customer):\")\n",
    "print(\"=\"*50)\n",
    "target_corr = correlation_matrix['high_value_customer'].sort_values(ascending=False)\n",
    "print(target_corr.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Customer Segmentation using Clustering (25 points)\n",
    "\n",
    "## üéØ Learning Goals:\n",
    "- Understand what clustering is\n",
    "- Use K-Means algorithm to group customers\n",
    "- Find the optimal number of clusters\n",
    "- Visualize and interpret customer segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is Clustering? ü§î\n",
    "\n",
    "**Clustering = Grouping similar things together**\n",
    "\n",
    "Imagine you have 1000 customers. Some spend a lot, some visit often, some buy many products. \n",
    "Clustering helps us group similar customers together automatically!\n",
    "\n",
    "**K-Means Algorithm:**\n",
    "1. Choose K (number of groups)\n",
    "2. Place K random points as \"centers\"\n",
    "3. Assign each customer to nearest center\n",
    "4. Move centers to average of their group\n",
    "5. Repeat steps 3-4 until groups stabilize\n",
    "\n",
    "**Why use it?**\n",
    "- Marketing: Target different customer groups differently\n",
    "- Business: Understand customer behavior patterns\n",
    "- Strategy: Personalize offers for each segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare Data for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for clustering\n",
    "# We'll use: spending, transactions, recency, and product variety\n",
    "clustering_features = [\n",
    "    'total_spend',\n",
    "    'num_transactions', \n",
    "    'avg_transaction_value',\n",
    "    'days_since_last_purchase',\n",
    "    'num_visits',\n",
    "    'product_categories_purchased',\n",
    "    'discount_used'\n",
    "]\n",
    "\n",
    "# Create clustering dataset\n",
    "X_cluster = df[clustering_features].copy()\n",
    "\n",
    "print(\"üìä Features selected for clustering:\")\n",
    "print(\"=\"*50)\n",
    "for i, feature in enumerate(clustering_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "print(f\"\\nShape: {X_cluster.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "X_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Scaling\n",
    "\n",
    "**Why scale features?**\n",
    "\n",
    "Imagine measuring distance between customers using:\n",
    "- Total spend: ‚Çπ100 to ‚Çπ10,000 (huge range!)\n",
    "- Number of transactions: 1 to 30 (small range)\n",
    "\n",
    "Without scaling, total_spend would dominate the clustering!\n",
    "\n",
    "**StandardScaler:**\n",
    "- Transforms each feature to have mean=0, std=1\n",
    "- Formula: (value - mean) / std\n",
    "- Now all features are on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "print(\"‚úÖ Features scaled successfully!\")\n",
    "print(\"\\nBefore scaling (first customer):\")\n",
    "print(X_cluster.iloc[0].to_dict())\n",
    "print(\"\\nAfter scaling (first customer):\")\n",
    "print(dict(zip(clustering_features, X_scaled[0])))\n",
    "print(\"\\nüìä Scaled data statistics:\")\n",
    "print(f\"Mean: {X_scaled.mean(axis=0).round(10)}\")\n",
    "print(f\"Std Dev: {X_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Find Optimal Number of Clusters\n",
    "\n",
    "**The Challenge:** How many groups should we create?\n",
    "\n",
    "**Method 1: Elbow Method**\n",
    "- Try different K values (2, 3, 4, 5...)\n",
    "- Calculate \"inertia\" (how tight the clusters are)\n",
    "- Plot K vs Inertia\n",
    "- Look for the \"elbow\" - where adding more clusters doesn't help much\n",
    "\n",
    "**Method 2: Silhouette Score**\n",
    "- Measures how well-separated clusters are\n",
    "- Range: -1 to +1\n",
    "- Higher = better separated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of clusters\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"üîç Testing different number of clusters...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for k in K_range:\n",
    "    # Create and fit K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={sil_score:.3f}\")\n",
    "\n",
    "# Find best K based on silhouette score\n",
    "best_k = K_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"\\n‚ú® Best K based on Silhouette Score: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualize Elbow and Silhouette Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow Plot\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia', fontsize=12)\n",
    "axes[0].set_title('Elbow Method\\n(Look for the \"elbow\" point)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette Plot\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].axvline(best_k, color='green', linestyle='--', linewidth=2, label=f'Best K={best_k}')\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score\\n(Higher is better)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Apply K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best K (or you can manually choose based on business needs)\n",
    "final_k = best_k  # You can change this if needed\n",
    "\n",
    "print(f\"üéØ Applying K-Means with K={final_k} clusters...\")\n",
    "\n",
    "# Create and fit final K-Means model\n",
    "kmeans_final = KMeans(n_clusters=final_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "print(\"\\n‚úÖ Clustering complete!\")\n",
    "print(\"\\nüìä Cluster Distribution:\")\n",
    "print(\"=\"*50)\n",
    "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"Cluster {cluster}: {count} customers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Analyze Cluster Characteristics\n",
    "\n",
    "Now let's understand what makes each cluster unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean values for each cluster\n",
    "cluster_profile = df.groupby('Cluster')[clustering_features].mean()\n",
    "\n",
    "print(\"üìä Cluster Profiles (Average Values):\")\n",
    "print(\"=\"*70)\n",
    "print(cluster_profile.round(2))\n",
    "\n",
    "# Create a more readable comparison\n",
    "print(\"\\nüéØ Cluster Characteristics:\")\n",
    "print(\"=\"*70)\n",
    "for cluster in range(final_k):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CLUSTER {cluster}:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    cluster_data = df[df['Cluster'] == cluster]\n",
    "    print(f\"üìà Size: {len(cluster_data)} customers ({len(cluster_data)/len(df)*100:.1f}%)\")\n",
    "    print(f\"üí∞ Avg Spend: ‚Çπ{cluster_data['total_spend'].mean():.2f}\")\n",
    "    print(f\"üõí Avg Transactions: {cluster_data['num_transactions'].mean():.1f}\")\n",
    "    print(f\"üìÖ Avg Days Since Purchase: {cluster_data['days_since_last_purchase'].mean():.1f}\")\n",
    "    print(f\"üè™ Avg Visits: {cluster_data['num_visits'].mean():.1f}\")\n",
    "    print(f\"üì¶ Avg Product Categories: {cluster_data['product_categories_purchased'].mean():.1f}\")\n",
    "    \n",
    "    # Determine cluster type\n",
    "    if cluster_data['total_spend'].mean() > df['total_spend'].median() and \\\n",
    "       cluster_data['days_since_last_purchase'].mean() < df['days_since_last_purchase'].median():\n",
    "        cluster_type = \"üåü High-Value Active Customers\"\n",
    "    elif cluster_data['total_spend'].mean() > df['total_spend'].median():\n",
    "        cluster_type = \"üíé High Spenders\"\n",
    "    elif cluster_data['days_since_last_purchase'].mean() > df['days_since_last_purchase'].median():\n",
    "        cluster_type = \"üò¥ Inactive/At-Risk Customers\"\n",
    "    else:\n",
    "        cluster_type = \"üë• Regular Customers\"\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è Cluster Type: {cluster_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for cluster visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Customer Segmentation Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Total Spend vs Num Transactions\n",
    "for cluster in range(final_k):\n",
    "    cluster_data = df[df['Cluster'] == cluster]\n",
    "    axes[0, 0].scatter(cluster_data['total_spend'], \n",
    "                       cluster_data['num_transactions'],\n",
    "                       label=f'Cluster {cluster}', \n",
    "                       alpha=0.6, s=50)\n",
    "axes[0, 0].set_xlabel('Total Spend', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Number of Transactions', fontsize=11)\n",
    "axes[0, 0].set_title('Spending vs Transaction Frequency', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Days Since Last Purchase vs Num Visits\n",
    "for cluster in range(final_k):\n",
    "    cluster_data = df[df['Cluster'] == cluster]\n",
    "    axes[0, 1].scatter(cluster_data['days_since_last_purchase'], \n",
    "                       cluster_data['num_visits'],\n",
    "                       label=f'Cluster {cluster}', \n",
    "                       alpha=0.6, s=50)\n",
    "axes[0, 1].set_xlabel('Days Since Last Purchase', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Number of Visits', fontsize=11)\n",
    "axes[0, 1].set_title('Recency vs Visit Frequency', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Average Transaction Value vs Product Categories\n",
    "for cluster in range(final_k):\n",
    "    cluster_data = df[df['Cluster'] == cluster]\n",
    "    axes[1, 0].scatter(cluster_data['avg_transaction_value'], \n",
    "                       cluster_data['product_categories_purchased'],\n",
    "                       label=f'Cluster {cluster}', \n",
    "                       alpha=0.6, s=50)\n",
    "axes[1, 0].set_xlabel('Average Transaction Value', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Product Categories Purchased', fontsize=11)\n",
    "axes[1, 0].set_title('Transaction Value vs Product Diversity', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Cluster Size\n",
    "cluster_counts.plot(kind='bar', ax=axes[1, 1], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'][:final_k])\n",
    "axes[1, 1].set_xlabel('Cluster', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Number of Customers', fontsize=11)\n",
    "axes[1, 1].set_title('Cluster Distribution', fontweight='bold')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Predictive Modeling (35 points)\n",
    "\n",
    "## üéØ Learning Goals:\n",
    "- Build classification models to predict high-value customers\n",
    "- Compare different algorithms\n",
    "- Evaluate model performance\n",
    "- Understand key metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What is Classification? ü§î\n",
    "\n",
    "**Classification = Predicting categories**\n",
    "\n",
    "In our case:\n",
    "- **Input**: Customer features (age, spending, etc.)\n",
    "- **Output**: Is this a high-value customer? (0 = No, 1 = Yes)\n",
    "\n",
    "**Why predict high-value customers?**\n",
    "- Target marketing campaigns\n",
    "- Allocate resources efficiently\n",
    "- Personalize offers\n",
    "- Prevent churn of valuable customers\n",
    "\n",
    "**Algorithms we'll try:**\n",
    "1. **Logistic Regression**: Simple, interpretable\n",
    "2. **Decision Tree**: Easy to understand, visual\n",
    "3. **Random Forest**: Powerful, combines many trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for prediction\n",
    "feature_columns = [\n",
    "    'age', 'total_spend', 'num_transactions', 'avg_transaction_value',\n",
    "    'days_since_last_purchase', 'num_visits', 'product_categories_purchased',\n",
    "    'discount_used', 'Cluster'\n",
    "]\n",
    "\n",
    "# Encode categorical variables\n",
    "df_model = df.copy()\n",
    "\n",
    "# Encode gender\n",
    "le_gender = LabelEncoder()\n",
    "df_model['gender_encoded'] = le_gender.fit_transform(df_model['gender'].fillna('Unknown'))\n",
    "\n",
    "# Encode city_tier\n",
    "le_city = LabelEncoder()\n",
    "df_model['city_tier_encoded'] = le_city.fit_transform(df_model['city_tier'])\n",
    "\n",
    "# Encode membership_type\n",
    "le_membership = LabelEncoder()\n",
    "df_model['membership_encoded'] = le_membership.fit_transform(df_model['membership_type'])\n",
    "\n",
    "# Add encoded features to feature list\n",
    "feature_columns.extend(['gender_encoded', 'city_tier_encoded', 'membership_encoded'])\n",
    "\n",
    "# Create feature matrix (X) and target vector (y)\n",
    "X = df_model[feature_columns]\n",
    "y = df_model['high_value_customer']\n",
    "\n",
    "print(\"‚úÖ Data prepared for modeling!\")\n",
    "print(f\"\\nüìä Feature Matrix Shape: {X.shape}\")\n",
    "print(f\"üéØ Target Variable Shape: {y.shape}\")\n",
    "print(f\"\\nüìà Target Distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nPercentage of high-value customers: {(y.sum()/len(y)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train-Test Split\n",
    "\n",
    "**Why split data?**\n",
    "- **Training Set (80%)**: Model learns from this\n",
    "- **Test Set (20%)**: Model evaluated on this (never seen before!)\n",
    "\n",
    "**Why not use all data for training?**\n",
    "- We need to know how well the model works on NEW data\n",
    "- Models can \"memorize\" training data (overfitting)\n",
    "- Test set simulates real-world unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # 20% for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Keep same proportion of 0s and 1s in both sets\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data split complete!\")\n",
    "print(\"\\nüìä Training Set:\")\n",
    "print(f\"  Features: {X_train.shape}\")\n",
    "print(f\"  Target: {y_train.shape}\")\n",
    "print(f\"  High-value %: {(y_train.sum()/len(y_train)*100):.1f}%\")\n",
    "\n",
    "print(\"\\nüìä Test Set:\")\n",
    "print(f\"  Features: {X_test.shape}\")\n",
    "print(f\"  Target: {y_test.shape}\")\n",
    "print(f\"  High-value %: {(y_test.sum()/len(y_test)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Scale Features\n",
    "\n",
    "Just like in clustering, we need to scale features for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler_model = StandardScaler()\n",
    "\n",
    "# Fit on training data only! (Important: avoid data leakage)\n",
    "X_train_scaled = scaler_model.fit_transform(X_train)\n",
    "X_test_scaled = scaler_model.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled successfully!\")\n",
    "print(\"\\n‚ö†Ô∏è Important: We fit the scaler ONLY on training data!\")\n",
    "print(\"   This prevents 'data leakage' from test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Build and Evaluate Models\n",
    "\n",
    "**Evaluation Metrics Explained:**\n",
    "\n",
    "1. **Accuracy**: (Correct Predictions) / (Total Predictions)\n",
    "   - Simple but can be misleading with imbalanced data\n",
    "\n",
    "2. **Precision**: Of all predicted high-value, how many actually are?\n",
    "   - Important when false positives are costly\n",
    "\n",
    "3. **Recall**: Of all actual high-value customers, how many did we catch?\n",
    "   - Important when false negatives are costly\n",
    "\n",
    "4. **F1-Score**: Harmonic mean of Precision and Recall\n",
    "   - Good overall metric\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "                 Predicted\n",
    "              No       Yes\n",
    "Actual  No    TN       FP\n",
    "        Yes   FN       TP\n",
    "```\n",
    "- TN (True Negative): Correctly predicted NOT high-value\n",
    "- TP (True Positive): Correctly predicted high-value\n",
    "- FN (False Negative): Missed a high-value customer\n",
    "- FP (False Positive): Wrongly predicted as high-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Training Logistic Regression...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and train model\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_lr_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"\\nüìä Logistic Regression Results:\")\n",
    "print(f\"  Accuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {lr_precision:.4f}\")\n",
    "print(f\"  Recall:    {lr_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {lr_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Not High-Value', 'High-Value']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not High-Value', 'High-Value'],\n",
    "            yticklabels=['Not High-Value', 'High-Value'])\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≥ Training Decision Tree...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and train model\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=20)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "y_pred_dt_proba = dt_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "dt_precision = precision_score(y_test, y_pred_dt)\n",
    "dt_recall = recall_score(y_test, y_pred_dt)\n",
    "dt_f1 = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"\\nüìä Decision Tree Results:\")\n",
    "print(f\"  Accuracy:  {dt_accuracy:.4f} ({dt_accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {dt_precision:.4f}\")\n",
    "print(f\"  Recall:    {dt_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {dt_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=['Not High-Value', 'High-Value']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Not High-Value', 'High-Value'],\n",
    "            yticklabels=['Not High-Value', 'High-Value'])\n",
    "plt.title('Confusion Matrix - Decision Tree', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≤ Training Random Forest...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and train model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=20)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_rf_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nüìä Random Forest Results:\")\n",
    "print(f\"  Accuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {rf_precision:.4f}\")\n",
    "print(f\"  Recall:    {rf_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {rf_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Not High-Value', 'High-Value']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['Not High-Value', 'High-Value'],\n",
    "            yticklabels=['Not High-Value', 'High-Value'])\n",
    "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'Accuracy': [lr_accuracy, dt_accuracy, rf_accuracy],\n",
    "    'Precision': [lr_precision, dt_precision, rf_precision],\n",
    "    'Recall': [lr_recall, dt_recall, rf_recall],\n",
    "    'F1-Score': [lr_f1, dt_f1, rf_f1]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(model_comparison.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = model_comparison['F1-Score'].idxmax()\n",
    "best_model_name = model_comparison.loc[best_model_idx, 'Model']\n",
    "best_f1 = model_comparison.loc[best_model_idx, 'F1-Score']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (F1-Score: {best_f1:.4f})\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(model_comparison))\n",
    "width = 0.2\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax.bar(x + i*width, model_comparison[metric], width, label=metric, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(model_comparison['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Feature Importance\n",
    "\n",
    "**Which features are most important for predictions?**\n",
    "\n",
    "Understanding this helps us:\n",
    "- Focus on collecting important data\n",
    "- Understand what drives high-value customers\n",
    "- Make business decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üîç Feature Importance (Random Forest):\")\n",
    "print(\"=\"*50)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10], color='steelblue')\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 10 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Model Optimization & Business Insights (20 points)\n",
    "\n",
    "## üéØ Learning Goals:\n",
    "- Fine-tune model parameters\n",
    "- Use cross-validation for robust evaluation\n",
    "- Extract business insights\n",
    "- Create actionable recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "**What are hyperparameters?**\n",
    "- Settings that control how the model learns\n",
    "- Examples: tree depth, number of trees, learning rate\n",
    "\n",
    "**GridSearchCV:**\n",
    "- Try different combinations of hyperparameters\n",
    "- Use cross-validation to find the best combination\n",
    "- Automatically selects the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Hyperparameter Tuning for Random Forest...\")\n",
    "print(\"=\"*70)\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Grid Search Complete!\")\n",
    "print(\"\\nüèÜ Best Parameters:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best Cross-Validation F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "best_rf_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with optimized model\n",
    "y_pred_optimized = best_rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "optimized_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "optimized_precision = precision_score(y_test, y_pred_optimized)\n",
    "optimized_recall = recall_score(y_test, y_pred_optimized)\n",
    "optimized_f1 = f1_score(y_test, y_pred_optimized)\n",
    "\n",
    "print(\"üìä Optimized Random Forest Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Accuracy:  {optimized_accuracy:.4f} ({optimized_accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {optimized_precision:.4f}\")\n",
    "print(f\"  Recall:    {optimized_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {optimized_f1:.4f}\")\n",
    "\n",
    "# Compare with original\n",
    "print(\"\\nüìà Improvement:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Accuracy:  {(optimized_accuracy - rf_accuracy)*100:+.2f}%\")\n",
    "print(f\"  Precision: {(optimized_precision - rf_precision)*100:+.2f}%\")\n",
    "print(f\"  Recall:    {(optimized_recall - rf_recall)*100:+.2f}%\")\n",
    "print(f\"  F1-Score:  {(optimized_f1 - rf_f1)*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cross-Validation Analysis\n",
    "\n",
    "**What is Cross-Validation?**\n",
    "- Split data into K parts (folds)\n",
    "- Train on K-1 parts, test on 1 part\n",
    "- Repeat K times with different test part\n",
    "- Average the results\n",
    "\n",
    "**Why use it?**\n",
    "- More reliable performance estimate\n",
    "- Uses all data for both training and testing\n",
    "- Reduces risk of lucky/unlucky split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Performing 5-Fold Cross-Validation...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(best_rf_model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "\n",
    "print(\"\\nüìä Cross-Validation Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Fold 1: {cv_scores[0]:.4f}\")\n",
    "print(f\"  Fold 2: {cv_scores[1]:.4f}\")\n",
    "print(f\"  Fold 3: {cv_scores[2]:.4f}\")\n",
    "print(f\"  Fold 4: {cv_scores[3]:.4f}\")\n",
    "print(f\"  Fold 5: {cv_scores[4]:.4f}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"  Mean F1-Score: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std Deviation: {cv_scores.std():.4f}\")\n",
    "print(f\"  95% Confidence Interval: [{cv_scores.mean() - 2*cv_scores.std():.4f}, {cv_scores.mean() + 2*cv_scores.std():.4f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, 'bo-', linewidth=2, markersize=10)\n",
    "plt.axhline(cv_scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.fill_between(range(1, 6), \n",
    "                 cv_scores.mean() - cv_scores.std(), \n",
    "                 cv_scores.mean() + cv_scores.std(), \n",
    "                 alpha=0.2, color='red')\n",
    "plt.xlabel('Fold Number', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Business Insights & Recommendations\n",
    "\n",
    "Now let's translate our technical findings into business value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*20 + \"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ 1. CUSTOMER SEGMENTATION INSIGHTS:\")\n",
    "print(\"-\" * 80)\n",
    "for cluster in range(final_k):\n",
    "    cluster_data = df[df['Cluster'] == cluster]\n",
    "    print(f\"\\n   Segment {cluster}: {len(cluster_data)} customers ({len(cluster_data)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   üí∞ Average Spending: ‚Çπ{cluster_data['total_spend'].mean():.2f}\")\n",
    "    print(f\"   üìÖ Recency: {cluster_data['days_since_last_purchase'].mean():.1f} days\")\n",
    "    print(f\"   üéØ High-Value %: {(cluster_data['high_value_customer'].sum()/len(cluster_data)*100):.1f}%\")\n",
    "\n",
    "print(\"\\n\\nüí° 2. KEY PREDICTIVE FACTORS:\")\n",
    "print(\"-\" * 80)\n",
    "top_features = feature_importance.head(5)\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\\nüìä 3. MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   ‚Ä¢ Our model can identify high-value customers with {optimized_accuracy*100:.1f}% accuracy\")\n",
    "print(f\"   ‚Ä¢ Precision: {optimized_precision*100:.1f}% (When we predict high-value, we're right {optimized_precision*100:.1f}% of the time)\")\n",
    "print(f\"   ‚Ä¢ Recall: {optimized_recall*100:.1f}% (We catch {optimized_recall*100:.1f}% of all high-value customers)\")\n",
    "\n",
    "print(\"\\n\\nüöÄ 4. ACTIONABLE RECOMMENDATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n   A. For Marketing Team:\")\n",
    "print(\"      ‚Ä¢ Target high-spend, high-frequency customers with premium offers\")\n",
    "print(\"      ‚Ä¢ Re-engage customers who haven't purchased in 40+ days\")\n",
    "print(\"      ‚Ä¢ Create campaigns for customers with high product category diversity\")\n",
    "\n",
    "print(\"\\n   B. For Sales Team:\")\n",
    "print(f\"      ‚Ä¢ Focus on the top {feature_importance.iloc[0]['Feature']} metric\")\n",
    "print(\"      ‚Ä¢ Prioritize customers with high transaction values\")\n",
    "print(\"      ‚Ä¢ Nurture customers with potential (medium spend, high frequency)\")\n",
    "\n",
    "print(\"\\n   C. For Customer Success:\")\n",
    "print(\"      ‚Ä¢ Monitor days since last purchase for at-risk customers\")\n",
    "print(\"      ‚Ä¢ Incentivize frequent visits to increase engagement\")\n",
    "print(\"      ‚Ä¢ Personalize experiences based on cluster characteristics\")\n",
    "\n",
    "print(\"\\n   D. For Product Team:\")\n",
    "print(\"      ‚Ä¢ Encourage cross-category purchases\")\n",
    "print(\"      ‚Ä¢ Optimize discount strategies based on customer value\")\n",
    "print(\"      ‚Ä¢ Design loyalty programs for high-frequency buyers\")\n",
    "\n",
    "print(\"\\n\\nüí∞ 5. EXPECTED BUSINESS IMPACT:\")\n",
    "print(\"-\" * 80)\n",
    "# Calculate potential revenue impact\n",
    "high_value_avg = df[df['high_value_customer'] == 1]['total_spend'].mean()\n",
    "regular_avg = df[df['high_value_customer'] == 0]['total_spend'].mean()\n",
    "value_diff = high_value_avg - regular_avg\n",
    "\n",
    "print(f\"   ‚Ä¢ High-value customers spend ‚Çπ{value_diff:.2f} more on average\")\n",
    "print(f\"   ‚Ä¢ By identifying and nurturing potential high-value customers:\")\n",
    "print(f\"     - Converting just 10% more customers to high-value\")\n",
    "print(f\"     - Could increase revenue by ‚Çπ{value_diff * len(df) * 0.10:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"END OF ANALYSIS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary: What You've Learned\n",
    "\n",
    "Congratulations! You've completed your first end-to-end data science project! Here's what you've mastered:\n",
    "\n",
    "### Part 1: Data Exploration & Preprocessing\n",
    "- ‚úÖ Load and explore datasets\n",
    "- ‚úÖ Handle missing values strategically\n",
    "- ‚úÖ Detect and understand outliers\n",
    "- ‚úÖ Visualize data distributions\n",
    "- ‚úÖ Analyze correlations\n",
    "\n",
    "### Part 2: Customer Segmentation\n",
    "- ‚úÖ Understand clustering concepts\n",
    "- ‚úÖ Apply K-Means algorithm\n",
    "- ‚úÖ Find optimal number of clusters\n",
    "- ‚úÖ Interpret customer segments\n",
    "- ‚úÖ Extract business insights\n",
    "\n",
    "### Part 3: Predictive Modeling\n",
    "- ‚úÖ Build classification models\n",
    "- ‚úÖ Compare multiple algorithms\n",
    "- ‚úÖ Evaluate with proper metrics\n",
    "- ‚úÖ Understand confusion matrices\n",
    "- ‚úÖ Identify important features\n",
    "\n",
    "### Part 4: Optimization & Insights\n",
    "- ‚úÖ Tune hyperparameters\n",
    "- ‚úÖ Use cross-validation\n",
    "- ‚úÖ Generate business recommendations\n",
    "- ‚úÖ Calculate business impact\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Experiment**: Try different feature combinations\n",
    "2. **Improve**: Test other algorithms (XGBoost, Neural Networks)\n",
    "3. **Deploy**: Think about how to use this model in production\n",
    "4. **Learn More**: Explore deep learning, NLP, computer vision\n",
    "\n",
    "## üìö Resources for Further Learning\n",
    "\n",
    "- **Scikit-learn Documentation**: https://scikit-learn.org/\n",
    "- **Kaggle**: Practice with real datasets\n",
    "- **Coursera/Udemy**: Structured courses\n",
    "- **Towards Data Science**: Articles and tutorials\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your data science journey! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
